{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Linear Algebra - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we shall apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. We shall follow the approach highlighted in previous lesson where we used numpy to build the appropriate matrices and vectors and solve for the $\\beta$ (unknown variables) vector. The beta vector will be used with test data to make new predictions. We shall also evaluate how good our model fit was. \n",
    "\n",
    "In order to make this experiment interesting. We shall use NumPy at every single stage of this experiment i.e. loading data, creating matrices, performing test train split, model fitting and evaluations.  \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Use linear algebra to apply simple regression modeling in Python and NumPy only\n",
    "* Apply train/test split using permutations in NumPy\n",
    "* Use matrix algebra with inverses and dot products to calculate the beta\n",
    "* Make predictions from the fitted model using previously unseen input features \n",
    "* Evaluate the fitted model by calculating the error between real and predicted values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # for reading csv file\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "The dataset we will use for this experiment is \"**Sales Prices in the City of Windsor, Canada**\", something very similar to the Boston Housing dataset. This dataset contains a number of input (independent) variables, including area, number of bedrooms/bathrooms, facilities(AC/garage) etc. and an output (dependent) variable, **price**. We shall formulate a linear algebra problem to find linear mappings from input to out features using the equation provided in the previous lesson. \n",
    "\n",
    "This will allow us to find a relationship between house features and house price for the given data, allowing us to find unknown prices for houses, given the input features.  \n",
    "\n",
    "A description of dataset and included features is available at [THIS LINK](https://rdrr.io/cran/Ecdat/man/Housing.html)\n",
    "\n",
    "In your repo, the dataset is available as `windsor_housing.csv` containing following variables:\n",
    "\n",
    "there are 11 input features (first 11 columns):\n",
    "\n",
    "\tlotsize\tbedrooms\tbathrms\tstories\tdriveway\trecroom\tfullbase\tgashw\tairco\tgaragepl\tprefarea\n",
    "\n",
    "and 1 output feature i.e. **price** (12th column). \n",
    "\n",
    "The focus of this lab is not really answering a preset analytical question, but to learn how we can perform a regression experiment, similar to one we performed in statsmodels, using mathematical manipulations. So we we wont be using any Pandas or statsmodels goodness here. The key objectives here are to a) understand regression with matrix algebra, and b) Mastery in NumPy scientific computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Prepare Data for Modeling \n",
    "\n",
    "Let's give you a head start by importing the dataset. We shall perform following steps to get the data ready for analysis:\n",
    "\n",
    "* Initialize an empty list `data` for loading data\n",
    "* Read the csv file containing complete (raw) `windsor_housing.csv`. [Use `csv.reader()` for loading data.](https://docs.python.org/3/library/csv.html). Store this in `data` one row at a time.\n",
    "\n",
    "* Drop the first row of csv file as it contains the names of variables (header) which won't be used during analysis (keeping this will cause errors as it contains text values).\n",
    "\n",
    "* Append a column of all 1s to the data (bias) as the first column\n",
    "\n",
    "* Convert `data` to a numpy array and inspect first few rows \n",
    "\n",
    "NOTE: `read.csv()` would read the csv as a text file, so we must convert the contents to float at some stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c5d84736ba45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code here\n",
    "data = []\n",
    "with open(\"windsor_housing.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        data.append([1] + row)\n",
    "\n",
    "data = np.array(data[1:])\n",
    "\n",
    "data = data.astype(float)\n",
    "\n",
    "# First 5 rows of raw data \n",
    "\n",
    "# array([[1.00e+00, 5.85e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n",
    "#         0.00e+00, 1.00e+00, 0.00e+00, 0.00e+00, 1.00e+00, 0.00e+00,\n",
    "#         4.20e+04],\n",
    "#        [1.00e+00, 4.00e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n",
    "#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
    "#         3.85e+04],\n",
    "#        [1.00e+00, 3.06e+03, 3.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n",
    "#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
    "#         4.95e+04],\n",
    "#        [1.00e+00, 6.65e+03, 3.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n",
    "#         1.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
    "#         6.05e+04],\n",
    "#        [1.00e+00, 6.36e+03, 2.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n",
    "#         0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
    "#         6.10e+04]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Perform a 80/20 test train Split\n",
    "\n",
    "Explore NumPy's official documentation to manually split a dataset using `numpy.random.shuffle()`,  `numpy.random.permutations()` or using simple resampling method. \n",
    "* Perform a **RANDOM** 80/20 split on data using a method of your choice , in NumPy using one of the methods above. \n",
    "* Create x_test, y_test, x_train and y_train arrays from the split data.\n",
    "* Inspect the contents to see if the split performed as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 13)\n",
      "(546, 12) (546,)\n",
      "(437, 12) (109, 12) (437,) (109,)\n"
     ]
    }
   ],
   "source": [
    "# Your code here \n",
    "import numpy as np\n",
    "\n",
    "eighty = round(len(data) * .80)\n",
    "\n",
    "np.random.seed(345)\n",
    "np.random.shuffle(data)\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "\n",
    "X_train = X[:eighty]\n",
    "X_test = X[eighty:]\n",
    "y_train = y[:eighty]\n",
    "y_test = y[eighty:]\n",
    "print(data.shape)\n",
    "print(X.shape, y.shape)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "# Split results\n",
    "\n",
    "# Raw data Shape:  (546, 13)\n",
    "# Train/Test Split: (437, 13) (109, 13)\n",
    "# x_train, y_train, x_test, y_test: (437, 12) (437,) (109, 12) (109,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate the `beta` \n",
    "\n",
    "With our X and y in place, We can now compute our beta values with x_train and y_train as:\n",
    "#### $\\beta$ = (x_train<sup>T</sup> . x_train)<sup>-1</sup> . x_train<sup>T</sup> . y_train \n",
    "\n",
    "* Using numpy operations (transpose, inverse) that we saw earlier, compute the above equation in steps.\n",
    "* Print your beta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.06367243e+03  3.30133737e+00  2.14584345e+03  1.45222451e+04\n",
      "  6.70414379e+03  6.08424917e+03  5.52605797e+03  5.08510135e+03\n",
      "  1.21178683e+04  1.30337930e+04  4.79416970e+03  9.78500754e+03]\n"
     ]
    }
   ],
   "source": [
    "# Your code here \n",
    "def beta(x, y):\n",
    "    return np.linalg.inv((x.T.dot(x))).dot(x.T.dot(y))\n",
    "\n",
    "b = beta(X_train, y_train)\n",
    "print(b)\n",
    "\n",
    "\n",
    "# Calculated beta values\n",
    "\n",
    "# [-3.07118956e+03  2.13543921e+00  4.04283395e+03  1.33559881e+04\n",
    "#   5.75279185e+03  7.82810082e+03  3.73584043e+03  6.51098935e+03\n",
    "#   1.28802060e+04  1.09853850e+04  6.14947126e+03  1.05813305e+04]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Make Predictions\n",
    "Great , we now have a set of coefficients that describe the linear mappings between X and y. We can now use the calculated beta values  with the test datasets that we left out to calculate y predictions. \n",
    "For this we need to perform the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall all features in each row in turn and multiply it with the beta computed above. The result will give a prediction for each row which we can append to a new array of predictions.\n",
    "\n",
    "#### $\\hat{y}$ = x.$\\beta$ = $\\beta$<sub>0</sub> + $\\beta$<sub>1</sub> . x<sub>1</sub> + $\\beta$<sub>2</sub> . x<sub>2</sub> + .. + $\\beta$<sub>m</sub> . x<sub>m</sub>\n",
    "\n",
    "\n",
    "* Create new empty list (y_pred) for saving predictions.\n",
    "* For each row of x_test, take the dot product of the row with beta to calculate the prediction for that row.\n",
    "* Append the predictions to y_pred.\n",
    "* Print the new set of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 56826.34704873  63778.03486821  69058.78746024  54388.15897585\n",
      "  36897.94400012  52958.95030765  49560.97590688  56041.25294995\n",
      "  51377.75326302  51611.82401292  46480.94374174  54388.15897585\n",
      "  86446.32394506  49558.54698015  89188.44150272  61855.02568892\n",
      "  51460.96256442  44146.80845512 100299.45678855  79898.63456372\n",
      "  49625.61552992 115173.30759122  45934.74618556  74682.49108201\n",
      "  78393.51465154  72633.26342186  59959.15979174  31358.41549395\n",
      "  98253.19522475  92107.51458959  40876.05553158  40526.11377031\n",
      "  82586.18727622  68997.92066266  87029.29842265  65168.36931287\n",
      "  52071.03411083  42155.72303159  46363.48023878  52928.31418893\n",
      "  50221.26921686  56361.70505426  40083.73456266  88803.34736979\n",
      "  71448.22226127  55504.45081203  79807.18605905  86317.61687448\n",
      " 100514.42976425  43220.00506464  67050.97488101  54227.40473433\n",
      "  40208.40273777  53755.43215881  56920.52931639  73457.15503958\n",
      "  56373.78881531  76215.56728848  40703.60334335  59680.94460567\n",
      "  63554.98120368  45887.74481741  84600.94727769  76979.50045461\n",
      " 108134.47869116  91937.50835398  45623.63782777  48535.15823116\n",
      "  91449.55675199 116440.89055575  39555.52058338  56597.65192323\n",
      "  70023.76417425  43913.28591245  62990.90353737  46538.05193081\n",
      " 117458.53435922  54298.36919769  80434.02277884  40876.05553158\n",
      "  73030.8612594   71972.99594776  76858.79347045  40909.06890529\n",
      "  77780.51987057  71146.27448076 107138.17294611  64290.50887292\n",
      "  51982.98427666  48768.65493796  54520.21247067 113164.51774777\n",
      "  67468.31945867  62405.58136902  49263.85554354  63059.80643398\n",
      "  37980.00001268  81723.80000958  42000.91332841  73874.29680844\n",
      "  63420.10323141  54580.05051241  66675.76984507  45530.941224\n",
      "  95982.90386968  34098.4996756   63916.13542756  46984.57146938\n",
      "  97160.9896932 ]\n"
     ]
    }
   ],
   "source": [
    "# Your code here \n",
    "y_pred = []\n",
    "for row in X_test:\n",
    "    y_pred.append(row.dot(b))\n",
    "y_pred = np.array(y_pred)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Model \n",
    "\n",
    "### Visualize Actual vs. Predicted\n",
    "This is exciting, so now our model can use the beta value to predict the price of houses given the input features. Let's plot these predictions against the actual values in y_test to see how much our model deviates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot predicted and actual values as line plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=[15, 10])\n",
    "\n",
    "plt.plot(y_pred, linestyle='-', marker='o', markerfacecolor=\"blue\", label='predictions', c=\"green\")\n",
    "plt.plot(y_test, linestyle='-', marker='o', label='actual values', c=\"magenta\")\n",
    "plt.title('Actual vs. predicted values')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look so bad, does it ? Our model, although isn't perfect at this stage, is making a good attempt to predict house prices although a few prediction seem a bit out. There could a number of reasons for this. Let's try to dig a bit deeper to check model's predictive abilities by comparing these prediction with actual values of y_test individually. That will help us calculate the RMSE value (Root Mean Squared Error) for our model. \n",
    "### Root Mean Squared Error\n",
    "Here is the formula for this again. \n",
    "\n",
    "![](rmse.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize an empty array `err`.\n",
    "* for each row in y_test and y_pred, take the squared difference and append error for each row in err array. \n",
    "* Calculate RMSE from `err` using the formula shown above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "15416.819783458828\n"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE\n",
    "err = (y_test - y_pred) ** 2\n",
    "\n",
    "RMSE = np.sqrt(sum(err)/len(y_test))\n",
    "\n",
    "RMSE2 = np.sqrt(err.mean())\n",
    "print(round(RMSE, 5) == round(RMSE2, 5))\n",
    "\n",
    "print(RMSE)\n",
    "\n",
    "# Due to random split, your answers may vary \n",
    "\n",
    "# RMSE = 16401.913562758735"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Root Mean Squared Error\n",
    "The above error is clearly in terms of the dependant variable i.e. the final house price. We can also use a normlized mean squared error in case of multiple regression which can be calculated from RMSE using following formula:\n",
    "\n",
    "* Calculate normalized Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nrmse.png\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09343527141490199"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate NRMSE\n",
    "\n",
    "NRMSE = RMSE/((y_train).max()-y_train.min())\n",
    "\n",
    "NRMSE\n",
    "\n",
    "# Due to random split, your answers may vary \n",
    "\n",
    "# 0.09940553674399233"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SO there it is. A complete multiple regression analysis using nothing but numpy. Having good programming skills in numpy would allow to dig deeper into analytical algorithms in machine learning and deep learning. Using matrix multiplication techniques we saw here, we can easily build a whole neural network from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level up - Optional \n",
    "\n",
    "* Calculated the R_squared and adjusted R_squared for above experiment. \n",
    "* Plot the residuals (similar to statsmodels) and comment on the variance and heteroscedascticity. \n",
    "* Run the experiment in statsmodels and compare the performance of both approaches in terms of computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15416.819783458835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.586921620498517"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)\n",
    "y_pred_lin = lin.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred_lin)**.5)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.677</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   84.16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 14 Mar 2019</td> <th>  Prob (F-statistic):</th> <td>2.12e-99</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:10:44</td>     <th>  Log-Likelihood:    </th> <td> -4829.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   437</td>      <th>  AIC:               </th> <td>   9682.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   425</td>      <th>  BIC:               </th> <td>   9731.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>-4063.6724</td> <td> 3729.377</td> <td>   -1.090</td> <td> 0.276</td> <td>-1.14e+04</td> <td> 3266.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    3.3013</td> <td>    0.379</td> <td>    8.706</td> <td> 0.000</td> <td>    2.556</td> <td>    4.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> 2145.8435</td> <td> 1144.646</td> <td>    1.875</td> <td> 0.062</td> <td> -104.028</td> <td> 4395.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td> 1.452e+04</td> <td> 1655.662</td> <td>    8.771</td> <td> 0.000</td> <td> 1.13e+04</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td> 6704.1438</td> <td> 1012.462</td> <td>    6.622</td> <td> 0.000</td> <td> 4714.088</td> <td> 8694.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td> 6084.2492</td> <td> 2301.319</td> <td>    2.644</td> <td> 0.009</td> <td> 1560.865</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td> 5526.0580</td> <td> 2055.497</td> <td>    2.688</td> <td> 0.007</td> <td> 1485.853</td> <td> 9566.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td> 5085.1014</td> <td> 1772.726</td> <td>    2.869</td> <td> 0.004</td> <td> 1600.700</td> <td> 8569.502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td> 1.212e+04</td> <td> 3444.731</td> <td>    3.518</td> <td> 0.000</td> <td> 5347.038</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td> 1.303e+04</td> <td> 1721.211</td> <td>    7.572</td> <td> 0.000</td> <td> 9650.647</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td> 4794.1697</td> <td>  934.874</td> <td>    5.128</td> <td> 0.000</td> <td> 2956.617</td> <td> 6631.722</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td> 9785.0075</td> <td> 1872.681</td> <td>    5.225</td> <td> 0.000</td> <td> 6104.137</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>75.873</td> <th>  Durbin-Watson:     </th> <td>   2.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 205.503</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.836</td> <th>  Prob(JB):          </th> <td>2.37e-45</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.913</td> <th>  Cond. No.          </th> <td>3.05e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.05e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.685\n",
       "Model:                            OLS   Adj. R-squared:                  0.677\n",
       "Method:                 Least Squares   F-statistic:                     84.16\n",
       "Date:                Thu, 14 Mar 2019   Prob (F-statistic):           2.12e-99\n",
       "Time:                        13:10:44   Log-Likelihood:                -4829.2\n",
       "No. Observations:                 437   AIC:                             9682.\n",
       "Df Residuals:                     425   BIC:                             9731.\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const      -4063.6724   3729.377     -1.090      0.276   -1.14e+04    3266.648\n",
       "x1             3.3013      0.379      8.706      0.000       2.556       4.047\n",
       "x2          2145.8435   1144.646      1.875      0.062    -104.028    4395.715\n",
       "x3          1.452e+04   1655.662      8.771      0.000    1.13e+04    1.78e+04\n",
       "x4          6704.1438   1012.462      6.622      0.000    4714.088    8694.199\n",
       "x5          6084.2492   2301.319      2.644      0.009    1560.865    1.06e+04\n",
       "x6          5526.0580   2055.497      2.688      0.007    1485.853    9566.263\n",
       "x7          5085.1014   1772.726      2.869      0.004    1600.700    8569.502\n",
       "x8          1.212e+04   3444.731      3.518      0.000    5347.038    1.89e+04\n",
       "x9          1.303e+04   1721.211      7.572      0.000    9650.647    1.64e+04\n",
       "x10         4794.1697    934.874      5.128      0.000    2956.617    6631.722\n",
       "x11         9785.0075   1872.681      5.225      0.000    6104.137    1.35e+04\n",
       "==============================================================================\n",
       "Omnibus:                       75.873   Durbin-Watson:                   2.022\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              205.503\n",
       "Skew:                           0.836   Prob(JB):                     2.37e-45\n",
       "Kurtosis:                       5.913   Cond. No.                     3.05e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.05e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = sm.OLS(y_train, X_train)\n",
    "results = reg.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45549313307766903\n",
      "0.3984407371697518\n"
     ]
    }
   ],
   "source": [
    "SSE = sum(err)\n",
    "SST = sum((y_pred - y_train.mean())**2)\n",
    "\n",
    "R2 = 1 - (SSE/SST)\n",
    "print(R2)\n",
    "\n",
    "nev = 11 # number of explanatory variables aka the independent variables not including the constant\n",
    "adj_R2 = 1 - ((1-R2)*len(y_pred) -1)/(len(y_pred) - nev - 1)\n",
    "print(adj_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "So there we have it. A predictive model for predicting house prices in a given dataset. Remember this is a very naive implementation of regression modeling. The purpose here was to get an introduction to the applications of linear algebra into machine learning and predictive analysis. We still have a number of shortcomings in our modeling approach and we can further apply a number of data modeling techniques to improve this model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
